{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Basics\")\n",
    "         .master(\"local[*]\")\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Driver* - maintains information about Spark application (SparkContext),  distributes and schedules work across executors, returns final processing output to a user \n",
    "\n",
    "\n",
    "*Executor* - carry out the actual work the driver assigned to them. Executors also report state of execution to the driver in a periodic way \n",
    "\n",
    "\n",
    "*Workers* - worker nodes of the cluster where the executors run\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Schema vs. provide Schema\n",
    "\n",
    "**Infer schema:**\n",
    "* spark reads particular amount of data and based on it decides on column types, adds overhead\n",
    "* No need to type complex schemas of some nested JSONs\n",
    "* OK for exploring the data (with no direct access to it)\n",
    "\n",
    "**Provide Schema:**\n",
    "* No need for additional read\n",
    "* It's hard sometimes to specify the proper nested schema\n",
    "\n",
    "**For Parquet files all column metadata is already in the metadata file, no need to specify it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading with Data in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on reading data with spark in general can be found:\n",
    "http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is Lazy by default - it doesn't compute results until an action is called, but we'll cover these parts later on. For a short read you can check [my short post](https://medium.com/uncle-data/tale-of-two-lazy-and-the-eager-9dc468a3055e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "extracted_data_dir = '/home/iceberg/notebooks/PyCon_LT_Workshop/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yellow_taxi_2022-02.parquet',\n",
       " 'dim_taxi_zones.csv',\n",
       " 'yellow_taxi_2022-03.parquet',\n",
       " 'dim_payments.csv',\n",
       " 'yellow_taxi_2022-01.parquet',\n",
       " 'dim_vendor.csv',\n",
       " 'dim_rates.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(extracted_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running simple read command will just read the metadata and will infer/read schema if it's provided in the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: bigint, tpep_pickup_datetime: timestamp_ntz, tpep_dropoff_datetime: timestamp_ntz, passenger_count: double, trip_distance: double, RatecodeID: double, store_and_fwd_flag: string, PULocationID: bigint, DOLocationID: bigint, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, airport_fee: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(extracted_data_dir+\"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_data = spark.read.parquet(extracted_data_dir+\"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_taxi_zones = spark.read.option(\"header\",True).csv(extracted_data_dir+\"dim_taxi_zones.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
