{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a very simple way I've explained different components of Spark in this [Blog Post](https://uncledata.substack.com/p/distributed-processing-as-candy-counting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Architecture](./pics/SparkArchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Glossary\n",
    "\n",
    "The following table summarizes terms youâ€™ll see used to refer to cluster concepts:\n",
    "\n",
    "| Term              | Meaning                                                                                                                                                                                                 |\n",
    "|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Application       | User program built on Spark. Consists of a driver program and executors on the cluster.                                                                                                                 |\n",
    "| Application jar   | A jar containing the user's Spark application. In some cases, users will want to create an \"uber jar\" containing their application along with its dependencies. The user's jar should never include Hadoop or Spark libraries, however, these will be added at runtime. |\n",
    "| Driver program    | The process running the main() function of the application and creating the SparkContext.                                                                                                              |\n",
    "| Cluster manager   | An external service for acquiring resources on the cluster (e.g., standalone manager, Mesos, YARN, Kubernetes).                                                                                         |\n",
    "| Deploy mode       | Distinguishes where the driver process runs. In \"cluster\" mode, the framework launches the driver inside of the cluster. In \"client\" mode, the submitter launches the driver outside of the cluster.     |\n",
    "| Worker node       | Any node that can run application code in the cluster.                                                                                                                                                  |\n",
    "| Executor          | A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.                                   |\n",
    "| Task              | A unit of work that will be sent to one executor.                                                                                                                                                       |\n",
    "| Job               | A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save, collect); you'll see this term used in the driver's logs.                              |\n",
    "| Stage             | Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you'll see this term used in the driver's logs.           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
